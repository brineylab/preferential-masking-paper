{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b3c6d5-c3d8-47d8-9f48-584f45a4cd27",
   "metadata": {},
   "source": [
    "# CoV Classifier Inference\n",
    "\n",
    "for AttCAT systematic data selection (Fig. 5D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cd187f-cda0-424d-a7b1-5c791ca5ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    EsmTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datasets\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    Sequence,\n",
    "    Value,\n",
    "    ClassLabel,\n",
    "    load_dataset,\n",
    ")\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd5c91-263e-4781-9b57-dcadb602a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0602ae-3056-4c4b-8087-e65dfe7f680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models and corresponding test data\n",
    "cov_dict = {\n",
    "    n: {\n",
    "        # replace with actual paths to models/test data\n",
    "        \"models\": {\n",
    "            \"uniform_250k\": f\"../models/uniform-250k_itr{n}_50ep_HD-CoV\",\n",
    "            \"uniform_350k\": f\"../models/uniform-350k_itr{n}_50ep_HD-CoV\",\n",
    "            \"preferential_250k\": f\"../models/preferential-250k_itr{n}_50ep_HD-CoV\",\n",
    "        },\n",
    "        \"data\": f'./train-test_splits/E_hd-0_cov-1_test{n}.csv',\n",
    "    }\n",
    "for n in range(5)}\n",
    "\n",
    "cov_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f2d704-1426-4dc4-b5f5-5e0062d3ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference on entire test set for all 3 models\n",
    "for i in tqdm(cov_dict.keys()):\n",
    "\n",
    "    # load test data\n",
    "    test_data = pd.read_csv(cov_dict[i][\"data\"])\n",
    "    test_preds = test_data.copy() # for storing prediction metrics\n",
    "    \n",
    "    class_labels = ClassLabel(names=['Healthy-donor', 'CoV-specific'])\n",
    "    n_classes = len(class_labels.names)\n",
    "    label2id = {\"Healthy-donor\": 0, \"CoV-specific\": 1}\n",
    "    id2label = {0: \"Healthy-donor\", 1: \"CoV-specific\"}\n",
    "    \n",
    "    # make huggingface dataset\n",
    "    dataset = datasets.Dataset.from_pandas(test_data)\n",
    "    dataset = dataset.cast_column(\"label\", class_labels)\n",
    "    \n",
    "    # filter for length (model has max length of 320 from training)\n",
    "    def filter_long_sequences(item):\n",
    "        return (len(item['h_sequence'])+len(item['l_sequence'])) <= 315 # allows 4 tokens (start, sep (which is 2 tokens long), end)\n",
    "    filtered = dataset.filter(filter_long_sequences)\n",
    "    \n",
    "    # tokenizer\n",
    "    tokenizer = EsmTokenizer.from_pretrained(\"../tokenizer/vocab.txt\")\n",
    "    \n",
    "    def preprocess_dataset(\n",
    "        batch, \n",
    "        tokenizer=None, \n",
    "        tokenizer_path=\"./tokenizer\", \n",
    "        separator=\"<cls><cls>\",\n",
    "        max_len=320\n",
    "    ) -> list:\n",
    "        \"\"\"\n",
    "        docstring\n",
    "        \"\"\"\n",
    "        # tokenize the H/L sequence pair\n",
    "        sequences = [h + separator + l for h, l in zip(batch[\"h_sequence\"], batch[\"l_sequence\"])]\n",
    "        tokenized = tokenizer(sequences, padding=\"max_length\", max_length=max_len)\n",
    "        batch[\"input_ids\"] = tokenized.input_ids\n",
    "        batch[\"attention_mask\"] = tokenized.attention_mask\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    # tokenize\n",
    "    tokenized_dataset = filtered.map(\n",
    "        preprocess_dataset,\n",
    "        fn_kwargs={\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"max_len\": 320,\n",
    "        },\n",
    "        batched=True,\n",
    "        remove_columns=[\"name\", \"h_sequence\", \"l_sequence\"]\n",
    "    )\n",
    "\n",
    "    # load each model\n",
    "    for model_id, model_path in cov_dict[i][\"models\"].items():\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "    \n",
    "        # predict on test set and get metrics\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            args=TrainingArguments(output_dir=\"./\", \n",
    "                                   report_to=\"none\"), # to turn off wandb logging\n",
    "            eval_dataset=tokenized_dataset,\n",
    "        )\n",
    "        logits, labels, metrics = trainer.predict(tokenized_dataset)\n",
    "        probabilities = torch.softmax(torch.from_numpy(logits), dim=1).detach().numpy()[:, -1]\n",
    "        predictions = np.argmax(logits, axis=1)\n",
    "        \n",
    "        del model # free up memory\n",
    "        \n",
    "        # categorize predictions\n",
    "        pred_data = []\n",
    "        for pred, prob, label, logit in zip(predictions, probabilities, labels, logits):\n",
    "            if pred == label == 1:\n",
    "                category = \"true_positive\"\n",
    "            elif pred == label == 0:\n",
    "                category = \"true_negative\"\n",
    "            elif pred == 1 and label == 0:\n",
    "                category = \"false_positive\"\n",
    "            else:\n",
    "                category = \"false_negative\"\n",
    "            pred_data.append(\n",
    "                {\n",
    "                    # \"label\": label,\n",
    "                    f\"{model_id}_prediction\": pred,\n",
    "                    f\"{model_id}_probability\": prob,\n",
    "                    f\"{model_id}_category\": category,\n",
    "                    f\"{model_id}_logits\": logit,\n",
    "                }\n",
    "            )\n",
    "        pred_df = pd.DataFrame(pred_data)\n",
    "        # pred_df[\"is_correct\"] = pred_df[\"prediction\"] == pred_df[\"label\"]\n",
    "        \n",
    "        # store predictive performance with its corresponding sequence\n",
    "        test_preds = pd.concat([test_preds, pred_df], axis=1)\n",
    "\n",
    "    # count predictions that are consistent across all models (for AttCAT systematic selection)\n",
    "    test_preds[\"consistent_prediction\"] = test_preds.apply(lambda x: x[\"uniform_250k_prediction\"] if (x[\"uniform_250k_prediction\"] == x[\"uniform_350k_prediction\"] == x[\"preferential_250k_prediction\"]) else None, axis = 1)\n",
    "    test_preds[\"consistent_category\"] = test_preds.apply(lambda x: x[\"uniform_250k_category\"] if (x[\"uniform_250k_category\"] == x[\"uniform_350k_category\"] == x[\"preferential_250k_category\"]) else None, axis = 1)\n",
    "\n",
    "    # average prediction probability\n",
    "    test_preds[\"avg_probability\"] = test_preds.apply(lambda x: x[[\"uniform_250k_probability\", \"uniform_350k_probability\", \"preferential_250k_probability\"]].mean(), axis = 1)\n",
    "    \n",
    "    # save as csv\n",
    "    test_preds.to_csv(f\"./results/CoV_predictions_itr{i}.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d2e68-36b7-4357-9a10-fe3d97c940ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
